CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)
Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)
VAL/TEST transforms: ToTensor + Normalize
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/vgg6_cifar/scripts/train_experiment.py:71: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
  warnings.warn(warn_msg)
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/vgg6_cifar/engine/trainer.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
[Epoch 001] lr=0.05028 train_loss=1.7543 acc=0.3492 val_loss=1.6085 acc=0.4252
[Epoch 002] lr=0.05057 train_loss=1.4310 acc=0.4816 val_loss=1.1918 acc=0.5674
[Epoch 003] lr=0.05085 train_loss=1.2220 acc=0.5630 val_loss=1.3216 acc=0.5376
[Epoch 004] lr=0.05114 train_loss=1.0474 acc=0.6332 val_loss=0.9784 acc=0.6514
[Epoch 005] lr=0.05142 train_loss=0.9292 acc=0.6776 val_loss=0.9100 acc=0.6746
[Epoch 006] lr=0.05170 train_loss=0.8281 acc=0.7147 val_loss=0.8320 acc=0.7110
[Epoch 007] lr=0.05199 train_loss=0.7544 acc=0.7434 val_loss=0.7715 acc=0.7314
[Epoch 008] lr=0.05227 train_loss=0.6899 acc=0.7629 val_loss=0.7306 acc=0.7532
[Epoch 009] lr=0.05256 train_loss=0.6457 acc=0.7791 val_loss=0.6928 acc=0.7582
[Epoch 010] lr=0.05284 train_loss=0.5934 acc=0.8001 val_loss=0.6577 acc=0.7706
[Epoch 011] lr=0.05313 train_loss=0.5623 acc=0.8092 val_loss=0.7400 acc=0.7566
[Epoch 012] lr=0.05341 train_loss=0.5294 acc=0.8214 val_loss=0.6811 acc=0.7730
[Epoch 013] lr=0.05369 train_loss=0.4960 acc=0.8313 val_loss=0.6665 acc=0.7814
[Epoch 014] lr=0.05398 train_loss=0.4855 acc=0.8378 val_loss=0.6182 acc=0.7932
[Epoch 015] lr=0.05426 train_loss=0.4622 acc=0.8449 val_loss=0.5378 acc=0.8066
[Epoch 016] lr=0.05455 train_loss=0.4410 acc=0.8512 val_loss=0.5754 acc=0.8054
[Epoch 017] lr=0.05483 train_loss=0.4236 acc=0.8572 val_loss=0.6069 acc=0.7960
[Epoch 018] lr=0.05511 train_loss=0.4091 acc=0.8629 val_loss=0.6107 acc=0.7960
[Epoch 019] lr=0.05540 train_loss=0.4073 acc=0.8628 val_loss=0.5080 acc=0.8238
[Epoch 020] lr=0.05568 train_loss=0.3967 acc=0.8673 val_loss=0.4772 acc=0.8400
[Epoch 021] lr=0.05597 train_loss=0.3738 acc=0.8742 val_loss=0.5141 acc=0.8266
[Epoch 022] lr=0.05625 train_loss=0.3706 acc=0.8738 val_loss=0.5011 acc=0.8326
[Epoch 023] lr=0.05653 train_loss=0.3653 acc=0.8762 val_loss=0.5624 acc=0.8122
[Epoch 024] lr=0.05682 train_loss=0.3557 acc=0.8806 val_loss=0.4916 acc=0.8382
[Epoch 025] lr=0.05710 train_loss=0.3525 acc=0.8806 val_loss=0.4565 acc=0.8424
[Epoch 026] lr=0.05739 train_loss=0.3413 acc=0.8863 val_loss=0.5076 acc=0.8230
[Epoch 027] lr=0.05767 train_loss=0.3360 acc=0.8865 val_loss=0.5424 acc=0.8150
[Epoch 028] lr=0.05795 train_loss=0.3355 acc=0.8883 val_loss=0.6140 acc=0.7994
[Epoch 029] lr=0.05824 train_loss=0.3268 acc=0.8901 val_loss=0.5177 acc=0.8274
[Epoch 030] lr=0.05852 train_loss=0.3173 acc=0.8928 val_loss=0.4684 acc=0.8420
[Epoch 031] lr=0.05881 train_loss=0.3161 acc=0.8938 val_loss=0.5521 acc=0.8218
[Epoch 032] lr=0.05909 train_loss=0.3182 acc=0.8937 val_loss=0.5462 acc=0.8186
[Epoch 033] lr=0.05938 train_loss=0.3043 acc=0.8971 val_loss=0.4847 acc=0.8434
[Epoch 034] lr=0.05966 train_loss=0.3052 acc=0.8961 val_loss=0.4446 acc=0.8472
[Epoch 035] lr=0.05994 train_loss=0.3013 acc=0.8967 val_loss=0.5210 acc=0.8226
[Epoch 036] lr=0.06023 train_loss=0.3006 acc=0.9001 val_loss=0.4293 acc=0.8554
[Epoch 037] lr=0.06051 train_loss=0.3003 acc=0.8984 val_loss=0.4862 acc=0.8376
[Epoch 038] lr=0.06080 train_loss=0.2902 acc=0.9013 val_loss=0.5091 acc=0.8398
[Epoch 039] lr=0.06108 train_loss=0.2912 acc=0.9015 val_loss=0.4633 acc=0.8422
[Epoch 040] lr=0.06136 train_loss=0.2937 acc=0.9008 val_loss=0.4559 acc=0.8474
FINAL TEST: loss=0.4051  top1_acc=0.8723

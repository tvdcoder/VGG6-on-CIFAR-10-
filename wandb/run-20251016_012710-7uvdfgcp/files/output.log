CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)
Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)
VAL/TEST transforms: ToTensor + Normalize
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/vgg6_cifar/scripts/train_experiment.py:71: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
  warnings.warn(warn_msg)
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/vgg6_cifar/engine/trainer.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
[Epoch 001] lr=0.02507 train_loss=1.7166 acc=0.3636 val_loss=1.4728 acc=0.4600
[Epoch 002] lr=0.02514 train_loss=1.3828 acc=0.5045 val_loss=1.3972 acc=0.5154
[Epoch 003] lr=0.02521 train_loss=1.1594 acc=0.5954 val_loss=1.0895 acc=0.6208
[Epoch 004] lr=0.02528 train_loss=1.0089 acc=0.6500 val_loss=0.9134 acc=0.6932
[Epoch 005] lr=0.02536 train_loss=0.8803 acc=0.6970 val_loss=0.8560 acc=0.7144
[Epoch 006] lr=0.02543 train_loss=0.7954 acc=0.7274 val_loss=1.1749 acc=0.6512
[Epoch 007] lr=0.02550 train_loss=0.7274 acc=0.7529 val_loss=0.8747 acc=0.7134
[Epoch 008] lr=0.02557 train_loss=0.6704 acc=0.7761 val_loss=0.7656 acc=0.7330
[Epoch 009] lr=0.02564 train_loss=0.6233 acc=0.7906 val_loss=0.7647 acc=0.7426
[Epoch 010] lr=0.02571 train_loss=0.5812 acc=0.8039 val_loss=0.6816 acc=0.7668
[Epoch 011] lr=0.02578 train_loss=0.5507 acc=0.8149 val_loss=0.5679 acc=0.8038
[Epoch 012] lr=0.02585 train_loss=0.5204 acc=0.8257 val_loss=0.5710 acc=0.8030
[Epoch 013] lr=0.02592 train_loss=0.4969 acc=0.8312 val_loss=0.6325 acc=0.7874
[Epoch 014] lr=0.02599 train_loss=0.4793 acc=0.8404 val_loss=0.4904 acc=0.8276
[Epoch 015] lr=0.02607 train_loss=0.4528 acc=0.8498 val_loss=0.4824 acc=0.8346
[Epoch 016] lr=0.02614 train_loss=0.4373 acc=0.8530 val_loss=0.5266 acc=0.8170
[Epoch 017] lr=0.02621 train_loss=0.4217 acc=0.8599 val_loss=0.4903 acc=0.8324
[Epoch 018] lr=0.02628 train_loss=0.4056 acc=0.8656 val_loss=0.4551 acc=0.8478
[Epoch 019] lr=0.02635 train_loss=0.3975 acc=0.8667 val_loss=0.5215 acc=0.8174
[Epoch 020] lr=0.02642 train_loss=0.3869 acc=0.8694 val_loss=0.4534 acc=0.8490
[Epoch 021] lr=0.02649 train_loss=0.3724 acc=0.8748 val_loss=0.4781 acc=0.8430
[Epoch 022] lr=0.02656 train_loss=0.3725 acc=0.8744 val_loss=0.4442 acc=0.8446
[Epoch 023] lr=0.02663 train_loss=0.3567 acc=0.8797 val_loss=0.4499 acc=0.8476
[Epoch 024] lr=0.02670 train_loss=0.3504 acc=0.8830 val_loss=0.4492 acc=0.8474
[Epoch 025] lr=0.02678 train_loss=0.3396 acc=0.8859 val_loss=0.4674 acc=0.8396
[Epoch 026] lr=0.02685 train_loss=0.3369 acc=0.8874 val_loss=0.4139 acc=0.8568
[Epoch 027] lr=0.02692 train_loss=0.3280 acc=0.8894 val_loss=0.5020 acc=0.8330
[Epoch 028] lr=0.02699 train_loss=0.3280 acc=0.8905 val_loss=0.4928 acc=0.8418
[Epoch 029] lr=0.02706 train_loss=0.3171 acc=0.8932 val_loss=0.4535 acc=0.8476
[Epoch 030] lr=0.02713 train_loss=0.3125 acc=0.8963 val_loss=0.4319 acc=0.8506
[Epoch 031] lr=0.02720 train_loss=0.3134 acc=0.8949 val_loss=0.4051 acc=0.8618
[Epoch 032] lr=0.02727 train_loss=0.3065 acc=0.8970 val_loss=0.4317 acc=0.8516
[Epoch 033] lr=0.02734 train_loss=0.2970 acc=0.9005 val_loss=0.4666 acc=0.8408
[Epoch 034] lr=0.02741 train_loss=0.2919 acc=0.9013 val_loss=0.4200 acc=0.8644
[Epoch 035] lr=0.02749 train_loss=0.2927 acc=0.9014 val_loss=0.3879 acc=0.8702
[Epoch 036] lr=0.02756 train_loss=0.2823 acc=0.9036 val_loss=0.5334 acc=0.8250
[Epoch 037] lr=0.02763 train_loss=0.2900 acc=0.9012 val_loss=0.4076 acc=0.8610
[Epoch 038] lr=0.02770 train_loss=0.2820 acc=0.9037 val_loss=0.3911 acc=0.8636
[Epoch 039] lr=0.02777 train_loss=0.2789 acc=0.9058 val_loss=0.3962 acc=0.8658
[Epoch 040] lr=0.02784 train_loss=0.2773 acc=0.9078 val_loss=0.4528 acc=0.8502
FINAL TEST: loss=0.3782  top1_acc=0.8749

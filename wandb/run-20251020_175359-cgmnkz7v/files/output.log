CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)
Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)
VAL/TEST transforms: ToTensor + Normalize
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/vgg6_cifar/scripts/train_experiment.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
  warnings.warn(warn_msg)
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/vgg6_cifar/engine/trainer.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/Users/santhoshtanay/Desktop/momeet/vgg6_cifar_full_bundle/.venv/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
[Epoch 001] lr=0.10114 train_loss=1.6718 acc=0.3792 val_loss=1.3518 acc=0.5198
[Epoch 002] lr=0.10227 train_loss=1.2371 acc=0.5558 val_loss=1.1499 acc=0.5882
[Epoch 003] lr=0.10341 train_loss=0.9979 acc=0.6478 val_loss=0.9766 acc=0.6512
[Epoch 004] lr=0.10455 train_loss=0.8703 acc=0.6932 val_loss=0.8902 acc=0.6832
[Epoch 005] lr=0.10568 train_loss=0.7961 acc=0.7221 val_loss=0.7732 acc=0.7236
[Epoch 006] lr=0.10682 train_loss=0.7305 acc=0.7448 val_loss=0.7734 acc=0.7214
[Epoch 007] lr=0.10795 train_loss=0.6728 acc=0.7673 val_loss=0.7270 acc=0.7444
[Epoch 008] lr=0.10909 train_loss=0.6323 acc=0.7805 val_loss=0.7892 acc=0.7314
[Epoch 009] lr=0.11023 train_loss=0.6119 acc=0.7867 val_loss=0.6118 acc=0.7882
[Epoch 010] lr=0.11136 train_loss=0.5790 acc=0.7994 val_loss=0.6627 acc=0.7706
[Epoch 011] lr=0.11250 train_loss=0.5569 acc=0.8079 val_loss=0.6282 acc=0.7862
[Epoch 012] lr=0.11364 train_loss=0.5335 acc=0.8151 val_loss=0.5673 acc=0.8046
[Epoch 013] lr=0.11477 train_loss=0.5225 acc=0.8198 val_loss=0.6093 acc=0.7908
[Epoch 014] lr=0.11591 train_loss=0.5086 acc=0.8257 val_loss=0.5525 acc=0.8090
[Epoch 015] lr=0.11705 train_loss=0.4869 acc=0.8327 val_loss=0.5093 acc=0.8218
[Epoch 016] lr=0.11818 train_loss=0.4696 acc=0.8377 val_loss=0.5453 acc=0.8080
[Epoch 017] lr=0.11932 train_loss=0.4730 acc=0.8381 val_loss=0.5372 acc=0.8168
[Epoch 018] lr=0.12045 train_loss=0.4525 acc=0.8448 val_loss=0.5024 acc=0.8288
[Epoch 019] lr=0.12159 train_loss=0.4448 acc=0.8471 val_loss=0.5609 acc=0.8126
[Epoch 020] lr=0.12273 train_loss=0.4417 acc=0.8489 val_loss=0.4746 acc=0.8314
[Epoch 021] lr=0.12386 train_loss=0.4309 acc=0.8520 val_loss=0.6076 acc=0.7956
[Epoch 022] lr=0.12500 train_loss=0.4159 acc=0.8554 val_loss=0.5441 acc=0.8122
[Epoch 023] lr=0.12614 train_loss=0.4177 acc=0.8554 val_loss=0.6686 acc=0.7882
[Epoch 024] lr=0.12727 train_loss=0.4126 acc=0.8580 val_loss=0.4834 acc=0.8316
[Epoch 025] lr=0.12841 train_loss=0.3965 acc=0.8631 val_loss=0.4708 acc=0.8360
[Epoch 026] lr=0.12955 train_loss=0.4043 acc=0.8599 val_loss=0.5915 acc=0.8046
[Epoch 027] lr=0.13068 train_loss=0.3942 acc=0.8638 val_loss=0.5445 acc=0.8106
[Epoch 028] lr=0.13182 train_loss=0.3868 acc=0.8668 val_loss=0.4584 acc=0.8406
[Epoch 029] lr=0.13295 train_loss=0.3758 acc=0.8707 val_loss=0.4970 acc=0.8362
[Epoch 030] lr=0.13409 train_loss=0.3769 acc=0.8695 val_loss=0.4956 acc=0.8292
[Epoch 031] lr=0.13523 train_loss=0.3700 acc=0.8713 val_loss=0.4507 acc=0.8406
[Epoch 032] lr=0.13636 train_loss=0.3637 acc=0.8754 val_loss=0.5089 acc=0.8278
[Epoch 033] lr=0.13750 train_loss=0.3648 acc=0.8750 val_loss=0.6125 acc=0.8018
[Epoch 034] lr=0.13864 train_loss=0.3574 acc=0.8772 val_loss=0.4725 acc=0.8374
[Epoch 035] lr=0.13977 train_loss=0.3564 acc=0.8773 val_loss=0.4396 acc=0.8530
[Epoch 036] lr=0.14091 train_loss=0.3624 acc=0.8759 val_loss=0.5302 acc=0.8190
[Epoch 037] lr=0.14205 train_loss=0.3561 acc=0.8776 val_loss=0.4829 acc=0.8352
[Epoch 038] lr=0.14318 train_loss=0.3480 acc=0.8801 val_loss=0.4394 acc=0.8498
[Epoch 039] lr=0.14432 train_loss=0.3516 acc=0.8791 val_loss=0.4870 acc=0.8370
[Epoch 040] lr=0.14545 train_loss=0.3455 acc=0.8795 val_loss=0.5325 acc=0.8152
FINAL TEST: loss=0.3905  top1_acc=0.8705

# CS6886W - System Engineering for Deep Learning
# Assignment 1: VGG6 on CIFAR-10 - Final Report

**Name:** Momeet Singh Ahluwalia
**Roll No:** CS24M527

**GitHub Repository Link:** [(https://github.com/tvdcoder/VGG6-on-CIFAR-10-.git)]

This repository is structured to satisfy the entire assignment:
- **Q1**: Baseline training with proper normalization & augmentations; final test accuracy; loss/accuracy curves.
- **Q2**: Experiments varying **activations**, **optimizers**, and **batch size / epochs / LR**.
- **Q3**: Plots — W&B parallel-coordinates, validation-accuracy vs step (scatter), and training/validation curves.
- **Q4**: Final best configuration (reproducible).
- **Q5**: Clean, modular code + README with exact commands; include seed; upload best checkpoint to GitHub.

## Environment & Setup

### 1. Dependencies
Install the required packages:
```bash
pip install -r requirements.txt
````

### 2\. Weights & Biases

Log in to your W\&B account to enable experiment tracking:

```bash
wandb login
```

### 3\. Performance Optimization (Crucial for Speed)

This code is optimized to run on **Apple Silicon (M-series) GPUs** by using the `mps` device and a highly efficient data pipeline.

**To get maximum performance, make this one-time code change:**

  * Open `vgg6_cifar/data/cifar10.py`.
  * Find your `DataLoader` definitions for `train_loader`, `val_loader`, and `test_loader`.
  * Add **`pin_memory=True`** and **`prefetch_factor=4`** to all three `DataLoader` instances.

**Example:**

```python
# Inside vgg6_cifar/data/cifar10.py
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True,     # <-- ADD THIS
    prefetch_factor=4    # <-- ADD THIS
)
```

-----

## Q1 — Baseline (a–c)

This command trains the baseline model for 60 epochs with a small batch size, as a reference.

```bash
python -m vgg6_cifar.scripts.train_baseline \
  --data_dir ./data \
  --out_dir ./runs/baseline \
  --epochs 60 --batch_size 128 --lr 0.1 --optimizer sgd --momentum 0.9 \
  --weight_decay 5e-4 --label_smoothing 0.0 \
  --aug_hflip --aug_crop --aug_cutout --aug_jitter \
  --amp --seed 42
```

The final test metrics are saved in `./runs/baseline/final_test_metrics.json`.

-----

## Q2 — Model Performance on Different Configurations

A full sweep was run to compare activations and optimizers. All experiments were run with a high-performance configuration (`--batch_size 512`, `--num_workers 8`) to ensure fast training.

### Reproducing the Full Sweep

You can reproduce the entire set of experiments by running the W\&B agent with the sweep ID.

```bash
# Replace with your sweep ID (e.g., tvdcoder/vgg6-cifar10-assignment/xxxxxxx)
wandb agent <YOUR_SWEEP_ID>
```

### Individual High-Performance Commands

The sweep agent runs variations of the commands below.

#### (a) Vary activation (Fast)

```bash
# Example: ReLU (Fast)
python -m vgg6_cifar.scripts.train_experiment \
  --data_dir ./data --out_dir ./runs/act_relu_fast \
  --activation relu --optimizer sgd --lr 0.4 --batch_size 512 --epochs 40 \
  --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb \
  --num_workers 8
```

*(Other activations like `silu`, `gelu`, `tanh`, and `sigmoid` were also run.)*

#### (b) Vary optimizer (Fast)

```bash
# Example: Adam (Fast)
python -m vgg6_cifar.scripts.train_experiment \
  --data_dir ./data --out_dir ./runs/opt_adam_fast \
  --activation relu --optimizer adam --lr 0.001 --batch_size 512 --epochs 40 \
  --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb \
  --num_workers 8
```

*(Other optimizers like `sgd`, `nesterov-sgd`, `adamw`, `rmsprop`, `nadam`, and `adagrad` were also run.)*

-----

## Q3 — Plots

As per the assignment instructions, all plots are generated automatically by the **Weights & Biases sweep dashboard**.

1.  Open the W\&B project page for this assignment.
2.  Navigate to the "Sweeps" tab.
3.  All required plots can be generated and screenshotted from this dashboard:
      * **(a) Parallel-Coordinate Plot:** This plot is generated by default.
      * **(b) Validation Accuracy vs. Step (Scatter Plot):** Add a new panel of type "Scatter Plot" and set X-axis = `Step`, Y-axis = `val_acc`.
      * **(c) Train/Val Curves:** The default panels for `train_acc`, `val_acc`, `train_loss`, and `val_loss` show all runs overlaid.

-----

## Q4 — Final Model Performance

Based on the W\&B parallel coordinates plot, the single best-performing configuration was identified.

  * **Best Activation:** `relu`
  * **Best Optimizer:** `sgd`
  * **Best Learning Rate:** `0.4`
  * **Best Batch Size:** `512`
  * **Epochs:** `40`

### Exact Command to Reproduce Best Model

This command re-runs the single best configuration.

```bash
python -m vgg6_cifar.scripts.train_experiment \
  --data_dir ./data --out_dir ./runs/final_best \
  --activation relu \
  --optimizer sgd \
  --lr 0.4 \
  --batch_size 512 \
  --epochs 40 \
  --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --wandb --seed 42 \
  --num_workers 8
```

The final metrics are saved in `./runs/final_best/final_test_metrics.json` and the trained model is saved as `./runs/final_best/best.pt`.

-----

## Q5 — Reproducibility & Repository

  - **Modular Code:** The project is structured into `models/`, `data/`, `engine/`, `utils/`, and `scripts/` for clarity.
  - **Reproducibility:** All commands are in this README. Dependencies are in `requirements.txt`. The seed is fixed via `--seed 42`.
  - **Trained Model:** The `best.pt` file from the final Q4 run is included in this repository.

<!-- end list -->

```
```